{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "167aabca-7353-4255-a667-3129171a3264",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, Row\n",
    "from pyspark.sql.functions import col, from_json, udf, pandas_udf, PandasUDFType\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.conf import SparkConf\n",
    "from datetime import datetime, timezone, timedelta\n",
    "from pyspark.sql.types import StringType, IntegerType, TimestampType, StructType, StructField, DoubleType, ArrayType\n",
    "import os\n",
    "import psycopg2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d9f8710f-8bce-458f-ae26-88993fa5078c",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEBUG_MODE = os.environ.get('APP_DEBUG_MODE', 'True').lower() in ('1', 't', 'true') # if True, doesn't use the cluster, doesn't save in db and collects and the end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2bd605dd-db21-4cff-9847-8cb5ace006dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "db_conn_string = os.environ.get('APP_CONN_STRING', \"postgresql://postgres/fastid-report?user=postgres&password=tappino2025\")\n",
    "db_jdbc_url = os.environ.get('APP_JDBC_URL', \"jdbc:postgresql://postgres:5432/fastid-report\")\n",
    "db_jdbc_properties = {\n",
    "    \"user\": os.environ.get('APP_JDBC_USERNAME', \"postgres\"),\n",
    "    \"password\": os.environ.get('APP_JDBC_PASSWORD', \"tappino2025\"),\n",
    "    \"driver\": os.environ.get('APP_JDBC_DRIVER', \"org.postgresql.Driver\")\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1c5d65b3-a699-48fb-a8ca-809d5fdadade",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "        #.master(\"spark://spark:7077\") \\\n",
    "spark = SparkSession.builder \\\n",
    "\t\t.appName(\"FastIDReport\") \\\n",
    "        .master(\"local[*]\" if DEBUG_MODE else \"spark://spark:7077\") \\\n",
    "\t\t.config(\"spark.jars.ivy\", \"/tmp\") \\\n",
    "\t\t.config(\"spark.sql.shuffle.partitions\", 10) \\\n",
    "\t\t.config(\"spark.local.dir\", \"/tmp\") \\\n",
    "        .config('spark.jars.packages', 'org.apache.spark:spark-sql-kafka-0-10_2.12:3.4.3,org.postgresql:postgresql:42.7.4,org.elasticsearch:elasticsearch-spark-30_2.12:8.17.1') \\\n",
    "\t\t.getOrCreate()\n",
    "\n",
    "        #.config(\"spark.sql.streaming.checkpointLocation\", \"/tmp\") \\\n",
    "spark.sparkContext.setLogLevel(\"INFO\")\n",
    "#spark.sparkContext.setCheckpointDir(\"/home/jovyan/work/spark-checkpoints\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "81214c2c-cc18-4648-b7a4-168fb9870c13",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRANSACTION_SCHEMA = StructType([\n",
    "    StructField(\"ID\", IntegerType(), True),\n",
    "    StructField(\"InsertTimestamp\", TimestampType(), True),\n",
    "    StructField(\"Log_Id\", IntegerType(), True),\n",
    "    StructField(\"InstanceCode\", StringType(), True),\n",
    "    StructField(\"InstanceType\", StringType(), True),\n",
    "    StructField(\"User_Id\", IntegerType(), True),\n",
    "    StructField(\"User_Name\", StringType(), True),\n",
    "    StructField(\"Timestamp_creation\", TimestampType(), True),\n",
    "    StructField(\"Timestamp_discovery\", TimestampType(), True),\n",
    "    StructField(\"Timestamp_profile\", TimestampType(), True),\n",
    "    StructField(\"ApplicationId\", IntegerType(), True),\n",
    "    StructField(\"Success\", StringType(), True),\n",
    "    StructField(\"Type\", StringType(), True),\n",
    "    StructField(\"VersionApi\", StringType(), True),\n",
    "    StructField(\"ServiceName\", StringType(), True),\n",
    "    StructField(\"AttrSet\", StringType(), True),\n",
    "    StructField(\"AttrClass\", StringType(), True),\n",
    "    StructField(\"AuthnRequest_IssueInstant\", TimestampType(), True),\n",
    "    StructField(\"AuthnRequest_Issuer\", StringType(), True),\n",
    "    StructField(\"Response_IssueInstant\", TimestampType(), True),\n",
    "    StructField(\"Response_Assertion_AuthnContextClassRef\", StringType(), True),\n",
    "    StructField(\"Response_Assertion_AttributeStatement\", StringType(), True),\n",
    "    StructField(\"Response_Issuer\", StringType(), True),\n",
    "    StructField(\"uniqueIdpIdentity\", StringType(), True),\n",
    "    StructField(\"Transaction_Id\", IntegerType(), True),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "026c875e",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "df_transactions_raw = spark.read \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"kafkaServer:9092\") \\\n",
    "    .option(\"subscribe\", \"transactions\") \\\n",
    "    .load() \\\n",
    "    .selectExpr(\"CAST(value AS STRING)\") \\\n",
    "    .select(from_json(col(\"value\"), TRANSACTION_SCHEMA).alias(\"data\")) \\\n",
    "    .select(\"data.*\")\n",
    "    \n",
    "df_transactions_raw.createOrReplaceTempView(\"transactions\")\n",
    "\n",
    "df_transactions = spark.sql(\"SELECT COUNT(*) FROM transactions\")\n",
    "\n",
    "df_transactions.show()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "85777755-58b2-4352-9a5a-774432895f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_transactions_raw = spark.readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"kafkaServer:9092\") \\\n",
    "    .option(\"subscribe\", \"transactions\") \\\n",
    "    .option(\"startingOffsets\", \"{\\\"transactions\\\":{\\\"0\\\":724000}}\") \\\n",
    "    .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5623f1c-91ad-4004-b025-0f2241d21a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_transactions = df_transactions_raw \\\n",
    "        .selectExpr(\"CAST(value AS STRING)\") \\\n",
    "        .select(from_json(col(\"value\"), TRANSACTION_SCHEMA).alias(\"data\")) \\\n",
    "        .select(\"data.*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "198291f0-da5b-4f48-a203-fdaeac55661b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_transactions = df_transactions \\\n",
    "    .repartition(\"User_Name\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85b9fa9f-f959-4a50-b1c0-0ef4b790566c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_transactions = df_transactions \\\n",
    "    .withColumn(\n",
    "        \"Date\",\n",
    "        F.when(F.year(\"Response_IssueInstant\") > 2000, F.col(\"Response_IssueInstant\"))\n",
    "         .otherwise(F.col(\"Timestamp_creation\"))\n",
    "    ) \\\n",
    "    .withColumn(\n",
    "        \"DateYearMonth\",\n",
    "        F.trunc(F.col(\"Date\"), \"MM\")\n",
    "    ) \\\n",
    "    .withColumn('DateYearMonth', F.col(\"DateYearMonth\").cast(TimestampType())) \\\n",
    "    .withWatermark(\"Date\", \"10 seconds\") \\\n",
    "    .withColumn(\"DateYear\", F.year(\"DateYearMonth\")) \\\n",
    "    .withColumn(\"DateMonth\", F.month(\"DateYearMonth\"))\n",
    "    #.withWatermark(\"DateYearMonth\", \"10 seconds\") \\"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e18a388-959c-48f6-842e-5bf927598f45",
   "metadata": {},
   "source": [
    "# Clienti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "a88d5d04-7b27-4079-8ae6-3b76c6604d2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clienti = df_transactions \\\n",
    "    .withColumn(\"Cliente\", F.col(\"User_Name\")) \\\n",
    "    .select(\"Cliente\") \\\n",
    "    .distinct()\n",
    "\n",
    "#df_clienti.writeStream.outputMode(\"update\").foreachBatch(lambda df, _: save_clienti(df)).start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b5c9efc-3f7e-4b8e-aedf-20208769ecae",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "query = df_clienti.writeStream \\\n",
    "\t.outputMode(\"append\") \\\n",
    "    .foreachBatch(lambda df, _:\n",
    "                  df.write.jdbc(url=db_jdbc_url, table='\"Clienti\"', mode=\"append\", properties=db_jdbc_properties)) \\\n",
    "    .start()\n",
    "\n",
    "if DEBUG_MODE:\n",
    "    try:\n",
    "        query.awaitTermination()\n",
    "    except KeyboardInterrupt:\n",
    "        query.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "219273ce-f1ea-4c69-b426-fb00fe9c65b3",
   "metadata": {},
   "source": [
    "# IdP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9cbf79b4-cc42-4306-a5a0-2bd8865a253f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_idps = df_transactions \\\n",
    "    .withColumn(\"prefix\", F.substring(F.col(\"uniqueIdpIdentity\"), 1, 4)) \\\n",
    "    .filter(~F.col(\"prefix\").isin(\"UII_\", \"TENV\") & F.col(\"Response_Issuer\").isNotNull()) \\\n",
    "    .select(\"Response_Issuer\", \"prefix\") \\\n",
    "    .distinct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d1c1c5df-cc39-4f0e-9ab2-f0a828869ea1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "query = df_idps.writeStream \\\n",
    "\t.outputMode(\"append\") \\\n",
    "    .foreachBatch(lambda df, _:\n",
    "                  df.write.jdbc(url=db_jdbc_url, table='\"IdP\"', mode=\"append\", properties=db_jdbc_properties)) \\\n",
    "    .start()\n",
    "\n",
    "if DEBUG_MODE:\n",
    "    try:\n",
    "        query.awaitTermination()\n",
    "    except KeyboardInterrupt:\n",
    "        query.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18d868d3",
   "metadata": {},
   "source": [
    "# Count by Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eb621e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = df_transactions \\\n",
    "    .groupBy(\"User_Name\", \"Type\") \\\n",
    "    .agg(F.count(\"*\").alias(\"Count\")) \\\n",
    "    .withColumn(\"Id\", F.concat(F.col(\"User_Name\"), F.lit(\"_\"), F.col(\"Type\"))) \\\n",
    "    .writeStream \\\n",
    "    .outputMode(\"update\") \\\n",
    "    .option(\"checkpointLocation\", \"/tmp/\") \\\n",
    "    .option(\"es.nodes\", \"elasticsearch\") \\\n",
    "    .option(\"es.mapping.id\", \"Id\") \\\n",
    "    .option(\"es.port\", \"9200\") \\\n",
    "    .format(\"es\") \\\n",
    "    .start(\"count-by-type\")\n",
    "\n",
    "if DEBUG_MODE:\n",
    "    try:\n",
    "        query.awaitTermination()\n",
    "    except KeyboardInterrupt:\n",
    "        query.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a047569-c2ab-478b-afb8-4effd5ac763b",
   "metadata": {},
   "source": [
    "# Interval Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e981557-2835-4885-9d79-39aafe6aa811",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_interval_count = df_transactions \\\n",
    "    .groupBy(\"User_Name\", F.window(\"Date\", \"5 minute\")) \\\n",
    "    .agg(F.count(\"*\").alias(\"TransactionsCount\")) \\\n",
    "    .withColumn(\"Date\", F.col(\"window.start\")) \\\n",
    "    .select(\"Date\", \"User_Name\", \"TransactionsCount\")\n",
    "\n",
    "# df_interval_count.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0e5673c-7f85-4255-aca1-dc76b3c6ea30",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def append_interval_count(df):\n",
    "\n",
    "    df.write.jdbc(url=db_jdbc_url, table='\"TransactionsCount\"', mode=\"append\", properties=db_jdbc_properties)\n",
    "\n",
    "    '''\n",
    "    transactions_df = df.collect()\n",
    "    transactions = [tuple(row) for row in transactions_df]\n",
    "\n",
    "    connection = psycopg2.connect(db_conn_string)\n",
    "    cursor = connection.cursor()\n",
    "    insert_query = \"\"\"\n",
    "        INSERT INTO \"DailyTransactions\" (\"User_Name\", \"Date\", \"TransactionsCount\")\n",
    "        VALUES %s\n",
    "        ON CONFLICT (\"User_Name\", \"Date\")\n",
    "        DO UPDATE SET \"TransactionsCount\" = EXCLUDED.\"TransactionsCount\";\n",
    "    \"\"\"\n",
    "\n",
    "    execute_values(cursor, insert_query, transactions)\n",
    "\n",
    "    connection.commit()\n",
    "    '''\n",
    "\n",
    "query = df_interval_count.writeStream \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .foreachBatch(lambda df, _: append_interval_count(df)) \\\n",
    "    .start()\n",
    "\n",
    "if DEBUG_MODE:\n",
    "    try:\n",
    "        query.awaitTermination()\n",
    "    except KeyboardInterrupt:\n",
    "        query.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "410759ff-6bb3-448e-b742-17d140c7edb6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "query = df_interval_count \\\n",
    "    .withColumn(\"key\", F.concat(F.col(\"User_Name\"), F.lit(\"_\"), F.col(\"Date\"))) \\\n",
    "    .select(F.col(\"key\"), F.to_json(F.struct(\"Date\", \"User_Name\", \"TransactionsCount\")).alias(\"value\")) \\\n",
    "    .writeStream \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"kafkaServer:9092\") \\\n",
    "    .option(\"checkpointLocation\", \"/tmp\") \\\n",
    "    .option(\"topic\", \"customer-transactions-count\") \\\n",
    "    .start()\n",
    "\n",
    "if DEBUG_MODE:\n",
    "    try:\n",
    "        query.awaitTermination()\n",
    "    except KeyboardInterrupt:\n",
    "        query.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9f84a58-2560-4920-abc4-d72cb820c616",
   "metadata": {},
   "source": [
    "# Online Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e521a2ce-0b06-44b6-8b26-6fa93b4240b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_transactions_count_raw = spark.readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"kafkaServer:9092\") \\\n",
    "    .option(\"subscribe\", \"customer-transactions-count\") \\\n",
    "    .option(\"startingOffsets\", \"earliest\") \\\n",
    "    .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "775d2ab2-e3d8-440a-a613-4a13dc0d250e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_transactions_count = df_transactions_count_raw \\\n",
    "\t.selectExpr(\"CAST(value AS STRING)\") \\\n",
    "\t.select(from_json(col(\"value\"), \"Date TIMESTAMP, User_Name STRING, TransactionsCount INT\").alias(\"data\")) \\\n",
    "\t.select(\"data.*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68863459-1cf9-48ea-9017-7ff4e96b0f84",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "df_transactions_count.writeStream \\\n",
    "    .format(\"console\") \\\n",
    "    .outputMode(\"update\") \\\n",
    "    .start() \\\n",
    "    .awaitTermination()\n",
    "#    .trigger(processingTime='1 hour') \\\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2caa5fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_transactions_daily.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71cb437d-3356-495f-bbcb-7d06c3745493",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.streaming.state import GroupStateTimeout\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.pipeline import Pipeline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def update_user_state(key, data, state):\n",
    "\n",
    "    def train_model_and_predict(transactions_count, toPredictDates):\n",
    "\n",
    "        def extract_features(dates):\n",
    "            return pd.DataFrame({\n",
    "                \"timestamp\": [int(datetime.timestamp(x)) for x in dates],\n",
    "                \"weekday\": [x.weekday() for x in dates],\n",
    "                \"is_weekend\": [1 if x.weekday() >= 5 else 0 for x in dates]\n",
    "            })\n",
    "\n",
    "        X = extract_features(transactions_count[\"Date\"])\n",
    "        y = transactions_count[\"TransactionsCount\"].values\n",
    "        \n",
    "        model = Pipeline([\n",
    "            (\"poly\", PolynomialFeatures(degree=2)),\n",
    "            (\"rf\", RandomForestRegressor(n_estimators=100, random_state=42))\n",
    "        ])\n",
    "        model.fit(X, y)\n",
    "        \n",
    "        X_new = extract_features(toPredictDates)\n",
    "        return model.predict(X_new)\n",
    "\n",
    "    if state.hasTimedOut: # never\n",
    "        state.remove()\n",
    "        \n",
    "    else:\n",
    "        for batch in data:\n",
    "\n",
    "            if state.exists:\n",
    "                (past_transactions, last_date) = state.get\n",
    "                print(\"got state\")                \n",
    "                df_past_transactions = pd.DataFrame.from_dict(past_transactions)\n",
    "            else:\n",
    "                df_past_transactions = pd.DataFrame([], columns=[\"Date\", \"TransactionsCount\"])\n",
    "                print(\"brand new state\")                \n",
    "                last_date = None\n",
    "\n",
    "            print(\"df_past_transactions\", df_past_transactions)\n",
    "\n",
    "            df = pd.DataFrame([], columns=[\"User_Name\", \"Date\", \"PredictedTransactionsCount\", \"IsAfterRetrain\"])\n",
    "            for index, row in batch.iterrows():                \n",
    "                \n",
    "                if len(df_past_transactions) > 1:\n",
    "                    predicted_counts = train_model_and_predict(df_past_transactions,\n",
    "                        [row[\"Date\"]] if last_date is None else [row[\"Date\"], last_date])\n",
    "                    \n",
    "                    records = [{\n",
    "                        \"User_Name\": row[\"User_Name\"],\n",
    "                        \"Date\": row[\"Date\"],\n",
    "                        \"PredictedTransactionsCount\": predicted_counts[0],\n",
    "                        \"IsAfterRetrain\": False\n",
    "                    }]\n",
    "                    if last_date is not None:\n",
    "                        records.append({\n",
    "                            \"User_Name\": row[\"User_Name\"],\n",
    "                            \"Date\": last_date,\n",
    "                            \"PredictedTransactionsCount\": predicted_counts[1],\n",
    "                            \"IsAfterRetrain\": True\n",
    "                        })\n",
    "\n",
    "                    df = pd.concat([df, pd.DataFrame.from_records(records)])\n",
    "\n",
    "\n",
    "                df_past_transactions = pd.concat([df_past_transactions, pd.DataFrame.from_records([{\n",
    "                    \"Date\": row[\"Date\"],\n",
    "                    \"TransactionsCount\": row[\"TransactionsCount\"]\n",
    "                }])])\n",
    "\n",
    "                df_past_transactions = df_past_transactions[df_past_transactions[\"Date\"] > row[\"Date\"] - timedelta(days=30)]\n",
    "\n",
    "                last_date = row[\"Date\"] # KISS\n",
    "\n",
    "            # we save all transactions in state\n",
    "            x = pd.Series(df_past_transactions['TransactionsCount'].values, index=df_past_transactions['Date']).to_dict()\n",
    "            state.update((x, last_date))\n",
    "        \n",
    "            yield df\n",
    "\n",
    "query = df_transactions_count \\\n",
    "    .withWatermark(\"Date\", \"10 seconds\") \\\n",
    "    .groupBy(\"User_Name\") \\\n",
    "    .applyInPandasWithState(\n",
    "        update_user_state,\n",
    "        \"User_Name STRING, Date TIMESTAMP, PredictedTransactionsCount INT, IsAfterRetrain BOOLEAN\",\n",
    "        \"TransactionsCount MAP<TIMESTAMP, INT>, LastDate TIMESTAMP\",\n",
    "        \"update\",\n",
    "        GroupStateTimeout.NoTimeout\n",
    "    ) \\\n",
    "    .withColumn(\"created_at\", F.current_timestamp()) \\\n",
    "    .writeStream \\\n",
    "    .outputMode(\"update\") \\\n",
    "    .foreachBatch(lambda df, _: df.write.jdbc(url=db_jdbc_url, table='\"PredictedTransactionsCount\"', mode=\"append\", properties=db_jdbc_properties)) \\\n",
    "    .start()\n",
    "'''\n",
    ".writeStream \\\n",
    "    .outputMode(\"update\") \\\n",
    "    .format(\"console\") \\\n",
    "    .start()'''\n",
    "\n",
    "if DEBUG_MODE:\n",
    "    try:\n",
    "        query.awaitTermination()\n",
    "    except KeyboardInterrupt:\n",
    "        query.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96ddc64d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not DEBUG_MODE:\n",
    "\tspark.streams.awaitAnyTermination()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
